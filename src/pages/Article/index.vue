<template>
    <Layout>
        <div :class="$style.content">
            <blockquote>
                <p>2019 年 11 月 14 日凌晨，在微软服务 23 年的微软全球执行副总裁沈向洋博士宣布离开微软；2020 年 3 月 5 日，清华大学在线上举行了活动“沈向洋双聘教授聘任仪式暨春风讲堂第四讲”，沈向洋博士离开微软后的首个去向得以确认。会后，沈向洋发表了主题为 “如何设计和构建负责任的 AI” 的全英文演讲，InfoQ 对重点内容进行了整理。</p>
            </blockquote>
            <p>由于受到疫情影响，沈向洋教授无法到达现场，本次活动采用了两地连线直播的形式，通过清华大学春风讲堂的 B 站账号对外直播。2005 年，沈向洋曾以双聘教师的身份在清华任教，并于 2015 年参与了清华、华盛顿大学、微软共同成立的全球创新学院（GIX）。</p>
            <p>因此，本次在线直播为续聘仪式，清华大学副校长杨斌主持了颁奖活动，校长邱勇在活动开始前进行了致辞，对沈向洋教授的加入表示欢迎。随后，沈向洋在致辞中表达了对中国人工智能发展的信心，同时也期待能够早日在清华园里，与师生们一起工作、生活。</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/e4/a0/e433057dc4c67a36e44cc5158dc606a0.jpg"
                    alt
                />
            </p>
            <center>清华大学校长邱勇向沈向洋正式颁发聘书</center>
            <p>仪式过后，沈向洋进行了一次线上授课演讲，分享了他对 AI 可解释性与 AI 偏见相关问题的研究与看法。</p>
            <p>由于演讲为全英文，InfoQ 整理了重点内容供广大开发者参考。</p>
            <h2>沈向洋：如何设计和构建负责任的 AI</h2>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/e3/6d/e31aff7ba65b4409c0fd6106271e956d.jpg"
                    alt
                />
            </p>
            <h3>话题 1：AI 的可解释性</h3>
            <p>之前，我们谈论的 AI 都是出现在科幻小说或电影里。实际上，AI 现在已经走进我们的日常生活，我们每天都在和 AI 打交道。但是，当这些 AI 应用到医疗、金融等领域中时，我们就需要更加谨慎地看待。</p>
            <p>如今，AI 已经可以做决定，这是 AI 过程中非常重要的一步，这就引出了我的第一部分内容：我们缺乏对 AI 所做决定的认知。</p>
            <h2>我们缺乏对 AI 所做决定的认知</h2>
            <p>AI 就像一个黑匣子，它们能自己做出决定，但是我们并不清楚其中的缘由。所以，我们目前需要做的就是打开这个“黑匣子”，了解 AI 想表达的意思和可能会做出的决定。这就是我们今天演讲的主题：如何设计和构建负责任的 AI。</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/55/50/557166efade2ee1e3411f07dc26d0050.jpg"
                    alt
                />
            </p>
            <p>AI 的发展需要遵循一定的基本原则，包括公正、透明、可信赖 &amp; 安全、隐私 &amp; 安全、适用范围广泛、负责。</p>
            <p>我们每次发现新的技术，都会面临同样的问题：如何让技术变得更加可靠、安全和负责任 。</p>
            <p>举个例子，当人类在生产电气零部件时，都会有相应的检查记录。一旦哪里出现问题，就可以对操作文件进行复盘，从而找出问题。但是，AI 是不相同的，AI 没有这样的一张检修表，我们往往不知道是哪个环节出现了问题。</p>
            <h2>构建具有可解释性的 AI</h2>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/ad/82/ad89d8c315abb7b0bc6bdbb02e4a5482.jpg"
                    alt
                />
            </p>
            <p>上图是一张模型可解释性的变化图片，横轴代表模型的可解释能力，纵轴表示预测的准确性。从横轴来看，越向右边延伸，我们得到的模型的可解释性越大。从纵轴来看，越向上延伸，系统预测的准确性越高。很多年前，我们就已经使用了这种线性模式，只不过那时并不称之为 AI。</p>
            <p>总之，模型是非常复杂的，解释起来十分困难。接下来，我们可以通过例子来证明下为什么这件事情如此复杂。</p>
            <h4>案例一：提取和比较</h4>
            <p>我们试图找出更多的数据来检测模型的准确度。2016 年，很多地方都推出了用于预测未来罪犯的软件，法庭在审判时已经开始用 AI 进行辅助判断。越是如此，人们就越会担心算法是否存在偏见，让我们通过一张图表来具体分析：</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/15/9e/1587350030477fdb8b78aa43daf54d9e.png"
                    alt
                />
            </p>
            <p>红色曲线代表从模型提取的情况，绿色曲线代表实际情况。例如，你是如何知道这个人将会再次犯罪呢？那这时来回顾下过去的数据，数据显示这个人有犯罪史，这个人过去犯得罪越多，以后就越有可能犯罪。这一定程度上与第一个图形显示的结果相吻合。</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/9c/fd/9c0f5846b0154edf5caa2f208beab1fd.jpg"
                    alt
                />
            </p>
            <p>从图上可以看出，美国本土居民犯罪率较高，对应地，重新犯罪比率也更高（红色显示），但实际上绿线显示却与之相反，人们印象中非洲裔美国人很容易犯罪，但实际上也并非如此，也就是说尽管是基于事实进行预测，也存在着一定的偏见，所以在训练这种数据时要格外谨慎。</p>
            <h4>案例二：局部解释和与模型无关的解释</h4>
            <p>我们如何透过复杂的模型了解其中的内容呢？一个复杂的模型就像黑匣子一样，我们向里面输入一些东西，就会得到一些东西。我们之所以无法理解模型是因为模型本身就非常复杂，晦涩难懂。</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/1e/2a/1ed9c03758c8e936afa1cd7c35de7a2a.png"
                    alt
                />
            </p>
            <p>有些人就会认为，这样难懂的模型就不追求整体解释，只需要局部可解释性，那么就会出现下列问题。</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/1f/8d/1fd33ddd15817beb0dc4af9004e3068d.jpg"
                    alt
                />
            </p>
            <p>正如上图，模型的识别准确率已经可以达到 5/6，但是我仍然不知道我要什么，到底是哈士奇还是狼。如果需要的是一只哈士奇，却把狼带回家，那麻烦就大了。你以为训练了一个非常强大的模型，实际上并非如此，这就是我一直在强调的：模型的可解释性十分重要。</p>
            <h3>话题 2：AI 的偏见</h3>
            <h4>案例一：对不同肤色的偏见</h4>
            <p>在任何时候，构建 AI 都离不开数据。需要了解偏见来自哪里，就需要知道数据的来源。在微软、IBM 和 Face ++ 制定的面部识别算法中，黑人女性比白人的面部识别准确率要低。</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/51/5e/514ab110c760cb498bb0af67a131185e.jpg"
                    alt
                />
            </p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/cb/79/cb3968cb9b04c1f3bf11e336581fcb79.jpg"
                    alt
                />
            </p>
            <p>从上图可以看出，对黑色女性人脸识别的错误率高达了 21.073，很多人表示这是难以接受的。所以，我们对这个模型进行了调整。三个月后，模型改善后得到了如下结果：</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/52/e7/52147ecd9b8ae27fb595d126eef95ae7.jpg"
                    alt
                />
            </p>
            <p>
                可以看出，经过再次训练后的模型，在识别不同肤色人种时准确率明显提高，许多分类错误率已经为 0.000，即便是黑人女性，识别错误率也降低至 1.9008。从不断的训练中，我们得到结论：
                <strong>这种偏见来自于训练采用的样本数据。</strong>
            </p>
            <p>
                基于以上问题，我们对微软 500 名机器学习领域工程师进行了调查，我们问他们如何改善机器学习系统？在经过调查后得出结论：
                <strong>如今机器学习工程师面临的最大问题之一是他们知道出了一些问题，但是不知道具体是哪里出了问题，也并不知道为什么会出现问题。</strong>
            </p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/32/4a/329b843ae8bd8253cb1e59b30299544a.png"
                    alt
                />
            </p>
            <p>当我们训练一个复杂或简单的模型，最终得到的结论是准确率为 73.8%，再深究每个训练的数据集时会发现，不同的肤色和性别，得到的准确率是不一样的。一些结果还比较令人满意，但也有一些结果差强人意。</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/94/c7/941c2924099ff1ffbae65a14fdbb13c7.png"
                    alt
                />
            </p>
            <p>所以，我们构建了一个系统来进行对比，看到底是哪里出了问题。</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/b4/0b/b4aa380af99f89e89034fa5c5658270b.jpg"
                    alt
                />
            </p>
            <p>传统机器学习系统是低级模式，而现在的模型带有错误可解释性，可以从整体视角，根据数据集不同的特征来判断哪里出现了问题，也可以从集群角度来了解到底为什么会出现这样的问题。基于这种模型，一旦出现问题时你可以复检样本数据集、模型来找出问题症结。</p>
            <h4>案例二：消除嵌入文本的偏见</h4>
            <p>我们使用了很多数据进行训练，列出了 27 种职业，包括会计、律师、教师、建筑师等。我们将一段话嵌入进去，然后发现系统识别出其职业为“教师”，但是，如果我们将段落中的某些单词进行修改，只改变很小的一部分 ，从“她”到“他”其他都没有改变，最终识别出的结果就从“教师”变成了“律师”。</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/4f/35/4fdcdf729f3fd157308d7b943b8f1635.jpg"
                    alt
                />
            </p>
            <p>这里就涉及到了文字嵌入几何学（如下图所示）：</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/1e/2a/1ed9c03758c8e936afa1cd7c35de7a2a.png"
                    alt
                />
            </p>
            <p>这个几何嵌入有两个属性：Proximity 和 Parallelism。我这里提出苹果和微软，大家就会联想到两家公司的成立者很伟大，都是很大的公司，这就是嵌入的内容。</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/b1/16/b134b1260ac2ad59e45c94a3d8e40416.jpg"
                    alt
                />
                <img
                    src="https://static001.infoq.cn/resource/image/15/1d/15ae9bd2c4751f92d3485d96e7dbaa1d.jpg"
                    alt
                />
            </p>
            <p>根据上图可见，如果某个单词更向下邻近 He，则表示为他；如果某个单词更向上，邻近 She，则表示为她。横轴可以看出，单词越向右，就越与性别无关；越向左，越与性别相关，例如妻子和丈夫，爸爸和妈妈。这里还可以看出，很多时候谈及时髦，通常是形容女性，而说起杰出，通常用来形容男性。</p>
            <p>
                <img
                    src="https://static001.infoq.cn/resource/image/37/2c/3781ba966029ba7df85fd3d6ed246c2c.png"
                    alt
                />
            </p>
            <p>那么，现在我们已经知道问题出在了哪里，就可以用上述模型来解决。但是，我们之后就会发现原来“时髦”、“杰出”、“天才”这类词语既可以形容男性，又可以形容女性。</p>
            <p>我们已经进入了 AI 时代，我们的生活与 AI 息息相关，我们是接触 AI 的第一代人群，无论喜欢与否都别无选择，但是我们能决定该用何种方式来构建 AI 以及使用 AI。</p>
            <p>最后，感谢大家收看！</p>
            <p>
                <strong>附录：沈向洋简介</strong>
            </p>
            <p>1996 年 11 月，沈向洋正式加入微软，并从此开始了在微软 23 年的职业生涯。作为在微软服务时间最长、职位最高的华人高管，23 年的时间里，沈向洋参与了许多重要的项目，也见证了微软发展的各个重要瞬间。</p>
            <p>1999 年，沈向洋回到北京参与创立微软中国研究院，并担任微软亚洲研究院计算组主任研究员，高级研究员；2004 年，他升任第三任微软亚洲研究院院长，兼首席科学家；2007 年，沈向洋升任微软全球资深副总裁，之后在 2013 年成为微软全球执行副总裁。2019 年 11 月 14 日凌晨，在微软服务 23 年的沈向洋宣布离开微软。</p>
            <p>沈向洋主要专注于计算机视觉、图形学、人机交互、统计学习、模式识别和机器人等方向的研究工作，是计算机视觉和图形学研究的世界级专家。</p>
        </div>
        <template v-slot:aside>
            <!-- <UserInfoCard /> -->
            <RecommendCard />
            <!-- <CatalogCard /> -->
        </template>
    </Layout>
</template>

<script>
import Layout from "@components/Layout";

// import UserInfoCard from "@components/Aside/UserInfo";
import RecommendCard from "@components/Aside/Recommend";
// import CatalogCard from "@components/Aside/Catalog";

export default {
    name: "Article",
    data() {
        return {};
    },
    components: {
        Layout,
        // UserInfoCard,
        RecommendCard
        // CatalogCard
    },
    mounted() {}
};
</script>

<style lang="scss" module>
.content {
    background: #fff;
    padding: 20px;
}
</style>

